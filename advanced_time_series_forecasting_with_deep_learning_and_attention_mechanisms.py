# -*- coding: utf-8 -*-
"""Advanced Time Series Forecasting with Deep Learning and Attention Mechanisms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ur3vZcT2txZGpt9x38IaG4RgxDU3_Hhz
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Deep Learning Libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Statistical Models
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)

# ============================================================================
# 1. DATA GENERATION AND PREPROCESSING
# ============================================================================

def generate_multivariate_time_series(n_samples=2000, n_features=5):
    """
    Generate synthetic multivariate time series with seasonal patterns,
    trends, and correlations between features.
    """
    print("Generating multivariate time series data...")

    t = np.arange(n_samples)
    data = np.zeros((n_samples, n_features))

    for i in range(n_features):
        # Trend component
        trend = 0.02 * t + 10 * (i + 1)

        # Seasonal components (different periods for each feature)
        seasonal1 = 5 * np.sin(2 * np.pi * t / (50 + i * 10))
        seasonal2 = 3 * np.cos(2 * np.pi * t / (100 + i * 20))

        # Random walk component
        random_walk = np.cumsum(np.random.randn(n_samples) * 0.5)

        # Combine components
        data[:, i] = trend + seasonal1 + seasonal2 + random_walk

    # Add correlations between features
    for i in range(1, n_features):
        data[:, i] += 0.3 * data[:, i-1]

    # Create DataFrame
    columns = [f'feature_{i+1}' for i in range(n_features)]
    df = pd.DataFrame(data, columns=columns)
    df['timestamp'] = pd.date_range(start='2020-01-01', periods=n_samples, freq='H')

    return df

def create_sequences(data, lookback=48, forecast_horizon=12):
    """
    Create sequences for sequence-to-sequence forecasting.

    Args:
        data: numpy array of shape (n_samples, n_features)
        lookback: number of historical time steps
        forecast_horizon: number of future time steps to predict

    Returns:
        X: input sequences (n_sequences, lookback, n_features)
        y: target sequences (n_sequences, forecast_horizon, n_features)
    """
    X, y = [], []

    for i in range(len(data) - lookback - forecast_horizon + 1):
        X.append(data[i:i+lookback])
        y.append(data[i+lookback:i+lookback+forecast_horizon])

    return np.array(X), np.array(y)

def preprocess_data(df, lookback=48, forecast_horizon=12, train_ratio=0.7, val_ratio=0.15):
    """
    Preprocess data: normalize, create sequences, and split into train/val/test.
    """
    print(f"\nPreprocessing data...")
    print(f"Lookback window: {lookback}, Forecast horizon: {forecast_horizon}")

    # Extract features (exclude timestamp)
    feature_columns = [col for col in df.columns if col != 'timestamp']
    data = df[feature_columns].values

    # Normalize data
    scaler = StandardScaler()
    data_normalized = scaler.fit_transform(data)

    # Create sequences
    X, y = create_sequences(data_normalized, lookback, forecast_horizon)

    # Split data
    n_samples = len(X)
    train_size = int(n_samples * train_ratio)
    val_size = int(n_samples * val_ratio)

    X_train = X[:train_size]
    y_train = y[:train_size]

    X_val = X[train_size:train_size+val_size]
    y_val = y[train_size:train_size+val_size]

    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]

    print(f"Train samples: {len(X_train)}, Val samples: {len(X_val)}, Test samples: {len(X_test)}")

    return X_train, y_train, X_val, y_val, X_test, y_test, scaler

# ============================================================================
# 2. PYTORCH DATASET
# ============================================================================

class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ============================================================================
# 3. BASELINE MODEL 1: SARIMAX
# ============================================================================

def train_sarimax_model(train_data, test_data, order=(2,1,2), seasonal_order=(1,1,1,24)):
    """
    Train SARIMAX model on univariate time series (first feature only for simplicity).
    """
    print("\n" + "="*80)
    print("Training SARIMAX Baseline Model")
    print("="*80)

    # Use only first feature for SARIMAX (univariate)
    train_series = train_data[:, 0, 0]  # First feature from sequences

    try:
        model = SARIMAX(train_series, order=order, seasonal_order=seasonal_order)
        fitted_model = model.fit(disp=False)

        # Forecast
        forecast_steps = len(test_data)
        forecast = fitted_model.forecast(steps=forecast_steps)

        print(f"SARIMAX model trained successfully")
        return fitted_model, forecast

    except Exception as e:
        print(f"SARIMAX training failed: {e}")
        print("Using simple ARIMA model instead...")

        # Fallback to simple ARIMA
        model = ARIMA(train_series, order=(2,1,2))
        fitted_model = model.fit()
        forecast_steps = len(test_data)
        forecast = fitted_model.forecast(steps=forecast_steps)

        return fitted_model, forecast

# ============================================================================
# 4. BASELINE MODEL 2: STANDARD LSTM/GRU
# ============================================================================

class StandardLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, forecast_horizon, dropout=0.2):
        super(StandardLSTM, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.forecast_horizon = forecast_horizon

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        self.fc = nn.Linear(hidden_size, input_size * forecast_horizon)
        self.input_size = input_size

    def forward(self, x):
        batch_size = x.size(0)

        # LSTM forward
        lstm_out, _ = self.lstm(x)

        # Use last hidden state
        last_hidden = lstm_out[:, -1, :]

        # Fully connected layer
        output = self.fc(last_hidden)

        # Reshape to (batch_size, forecast_horizon, input_size)
        output = output.view(batch_size, self.forecast_horizon, self.input_size)

        return output

# ============================================================================
# 5. ADVANCED MODEL: LSTM WITH ATTENTION MECHANISM
# ============================================================================

class ScaledDotProductAttention(nn.Module):
    def __init__(self, hidden_size):
        super(ScaledDotProductAttention, self).__init__()
        self.hidden_size = hidden_size

    def forward(self, query, key, value):
        """
        Scaled Dot-Product Attention

        Args:
            query: (batch_size, hidden_size)
            key: (batch_size, seq_len, hidden_size)
            value: (batch_size, seq_len, hidden_size)

        Returns:
            context: (batch_size, hidden_size)
            attention_weights: (batch_size, seq_len)
        """
        # Calculate attention scores
        # query: (batch_size, 1, hidden_size)
        query = query.unsqueeze(1)

        # scores: (batch_size, 1, seq_len)
        scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(self.hidden_size)

        # Apply softmax
        attention_weights = torch.softmax(scores, dim=-1)

        # Calculate context vector
        # context: (batch_size, 1, hidden_size)
        context = torch.bmm(attention_weights, value)

        # Remove middle dimension
        context = context.squeeze(1)
        attention_weights = attention_weights.squeeze(1)

        return context, attention_weights

class AttentionLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, forecast_horizon, dropout=0.2):
        super(AttentionLSTM, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.forecast_horizon = forecast_horizon
        self.input_size = input_size

        # LSTM encoder
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # Attention mechanism
        self.attention = ScaledDotProductAttention(hidden_size)

        # Fully connected layers
        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)
        self.fc2 = nn.Linear(hidden_size, input_size * forecast_horizon)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, return_attention=False):
        batch_size = x.size(0)

        # LSTM encoding
        lstm_out, (hidden, cell) = self.lstm(x)

        # Get last hidden state as query
        query = lstm_out[:, -1, :]

        # Apply attention
        context, attention_weights = self.attention(query, lstm_out, lstm_out)

        # Concatenate context and last hidden state
        combined = torch.cat([query, context], dim=1)

        # Fully connected layers
        out = self.fc1(combined)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)

        # Reshape to (batch_size, forecast_horizon, input_size)
        output = out.view(batch_size, self.forecast_horizon, self.input_size)

        if return_attention:
            return output, attention_weights
        return output

# ============================================================================
# 6. TRAINING FUNCTIONS
# ============================================================================

def train_model(model, train_loader, val_loader, epochs=50, learning_rate=0.001, device='cpu'):
    """
    Train PyTorch model with early stopping.
    """
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0

    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)
        train_losses.append(train_loss)

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                output = model(X_batch)
                loss = criterion(output, y_batch)
                val_loss += loss.item()

        val_loss /= len(val_loader)
        val_losses.append(val_loss)

        # Print progress
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Save best model
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                model.load_state_dict(best_model_state)
                break

    return model, train_losses, val_losses

# ============================================================================
# 7. EVALUATION FUNCTIONS
# ============================================================================

def calculate_metrics(y_true, y_pred):
    """
    Calculate RMSE, MAE, and MAPE.
    """
    rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))
    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())

    # MAPE (avoid division by zero)
    mask = y_true.flatten() != 0
    mape = np.mean(np.abs((y_true.flatten()[mask] - y_pred.flatten()[mask]) / y_true.flatten()[mask])) * 100

    return rmse, mae, mape

def evaluate_model(model, test_loader, device='cpu'):
    """
    Evaluate model on test set.
    """
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(device)
            output = model(X_batch)
            predictions.append(output.cpu().numpy())
            actuals.append(y_batch.numpy())

    predictions = np.concatenate(predictions, axis=0)
    actuals = np.concatenate(actuals, axis=0)

    return predictions, actuals

# ============================================================================
# 8. VISUALIZATION FUNCTIONS
# ============================================================================

def visualize_attention_weights(model, X_sample, device='cpu', lookback=48):
    """
    Visualize attention weights for a single sample.
    """
    model.eval()
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_sample).unsqueeze(0).to(device)
        output, attention_weights = model(X_tensor, return_attention=True)
        attention_weights = attention_weights.cpu().numpy()[0]

    plt.figure(figsize=(12, 4))
    plt.plot(range(lookback), attention_weights)
    plt.xlabel('Time Step (Historical)')
    plt.ylabel('Attention Weight')
    plt.title('Attention Weights Across Historical Time Steps')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('attention_weights.png', dpi=300, bbox_inches='tight')
    plt.show()

    return attention_weights

def plot_predictions(y_true, y_pred, title, sample_idx=0, feature_idx=0):
    """
    Plot predictions vs actual values.
    """
    plt.figure(figsize=(12, 6))

    plt.plot(y_true[sample_idx, :, feature_idx], label='Actual', marker='o')
    plt.plot(y_pred[sample_idx, :, feature_idx], label='Predicted', marker='s')

    plt.xlabel('Forecast Horizon (Time Steps)')
    plt.ylabel('Value')
    plt.title(f'{title} - Feature {feature_idx+1}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{title.lower().replace(" ", "_")}.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_comparison_table(results_dict):
    """
    Create a comparison table of model performances.
    """
    df_results = pd.DataFrame(results_dict).T
    df_results = df_results.round(4)

    print("\n" + "="*80)
    print("MODEL PERFORMANCE COMPARISON")
    print("="*80)
    print(df_results.to_string())
    print("="*80)

    return df_results

# ============================================================================
# 9. MAIN EXECUTION
# ============================================================================

def main():
    print("="*80)
    print("ADVANCED TIME SERIES FORECASTING WITH ATTENTION MECHANISMS")
    print("="*80)

    # Hyperparameters
    LOOKBACK = 48
    FORECAST_HORIZON = 12
    HIDDEN_SIZE = 64
    NUM_LAYERS = 2
    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 0.001
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    print(f"\nDevice: {DEVICE}")
    print(f"Hyperparameters:")
    print(f"  - Lookback: {LOOKBACK}")
    print(f"  - Forecast Horizon: {FORECAST_HORIZON}")
    print(f"  - Hidden Size: {HIDDEN_SIZE}")
    print(f"  - Num Layers: {NUM_LAYERS}")
    print(f"  - Batch Size: {BATCH_SIZE}")
    print(f"  - Learning Rate: {LEARNING_RATE}")

    # Step 1: Generate and preprocess data
    df = generate_multivariate_time_series(n_samples=2000, n_features=5)
    X_train, y_train, X_val, y_val, X_test, y_test, scaler = preprocess_data(
        df, lookback=LOOKBACK, forecast_horizon=FORECAST_HORIZON
    )

    # Create DataLoaders
    train_dataset = TimeSeriesDataset(X_train, y_train)
    val_dataset = TimeSeriesDataset(X_val, y_val)
    test_dataset = TimeSeriesDataset(X_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

    input_size = X_train.shape[2]

    # Step 2: Train SARIMAX baseline
    sarimax_model, sarimax_forecast = train_sarimax_model(X_train, X_test)

    # Prepare SARIMAX predictions for comparison (only first feature, first horizon)
    sarimax_pred = np.zeros_like(y_test)
    sarimax_pred[:, 0, 0] = sarimax_forecast[:len(y_test)]

    # Step 3: Train Standard LSTM
    print("\n" + "="*80)
    print("Training Standard LSTM Model")
    print("="*80)

    lstm_model = StandardLSTM(input_size, HIDDEN_SIZE, NUM_LAYERS, FORECAST_HORIZON)
    lstm_model, lstm_train_losses, lstm_val_losses = train_model(
        lstm_model, train_loader, val_loader,
        epochs=EPOCHS, learning_rate=LEARNING_RATE, device=DEVICE
    )

    # Evaluate LSTM
    lstm_predictions, lstm_actuals = evaluate_model(lstm_model, test_loader, device=DEVICE)

    # Step 4: Train Attention-based LSTM
    print("\n" + "="*80)
    print("Training Attention-Based LSTM Model")
    print("="*80)

    attention_model = AttentionLSTM(input_size, HIDDEN_SIZE, NUM_LAYERS, FORECAST_HORIZON)
    attention_model, attn_train_losses, attn_val_losses = train_model(
        attention_model, train_loader, val_loader,
        epochs=EPOCHS, learning_rate=LEARNING_RATE, device=DEVICE
    )

    # Evaluate Attention model
    attention_predictions, attention_actuals = evaluate_model(attention_model, test_loader, device=DEVICE)

    # Step 5: Calculate metrics
    print("\n" + "="*80)
    print("CALCULATING METRICS")
    print("="*80)

    # SARIMAX metrics (only for first feature)
    sarimax_rmse, sarimax_mae, sarimax_mape = calculate_metrics(
        y_test[:, 0, 0:1], sarimax_pred[:, 0, 0:1]
    )

    # LSTM metrics
    lstm_rmse, lstm_mae, lstm_mape = calculate_metrics(lstm_actuals, lstm_predictions)

    # Attention LSTM metrics
    attention_rmse, attention_mae, attention_mape = calculate_metrics(
        attention_actuals, attention_predictions
    )

    # Create comparison table
    results = {
        'SARIMAX (Baseline)': {
            'RMSE': sarimax_rmse,
            'MAE': sarimax_mae,
            'MAPE': sarimax_mape
        },
        'Standard LSTM': {
            'RMSE': lstm_rmse,
            'MAE': lstm_mae,
            'MAPE': lstm_mape
        },
        'Attention LSTM': {
            'RMSE': attention_rmse,
            'MAE': attention_mae,
            'MAPE': attention_mape
        }
    }

    results_df = plot_comparison_table(results)

    # Step 6: Visualizations
    print("\n" + "="*80)
    print("GENERATING VISUALIZATIONS")
    print("="*80)

    # Plot predictions
    plot_predictions(lstm_actuals, lstm_predictions, 'Standard LSTM Predictions', sample_idx=0)
    plot_predictions(attention_actuals, attention_predictions, 'Attention LSTM Predictions', sample_idx=0)

    # Visualize attention weights
    print("\nGenerating attention weight visualization...")
    sample_idx = 0
    attention_weights = visualize_attention_weights(
        attention_model, X_test[sample_idx], device=DEVICE, lookback=LOOKBACK
    )

    print("\n" + "="*80)
    print("ATTENTION ANALYSIS")
    print("="*80)
    print(f"\nTop 5 most important historical time steps:")
    top_indices = np.argsort(attention_weights)[-5:][::-1]
    for rank, idx in enumerate(top_indices, 1):
        print(f"  {rank}. Time step {idx} (weight: {attention_weights[idx]:.4f})")

    print("\n" + "="*80)
    print("PROJECT COMPLETE!")
    print("="*80)
    print("\nGenerated files:")
    print("  - attention_weights.png")
    print("  - standard_lstm_predictions.png")
    print("  - attention_lstm_predictions.png")

    return results_df, attention_model, lstm_model

if __name__ == "__main__":
    results_df, attention_model, lstm_model = main()